<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Papers about Dynamic Gaussian Splatting</title>
      <link href="/2024/06/24/Papers-about-Dynamic-Gaussian-Splatting/"/>
      <url>/2024/06/24/Papers-about-Dynamic-Gaussian-Splatting/</url>
      
        <content type="html"><![CDATA[<h1 id="GaussianFlow-Splatting-Gaussian-Dynamics-for-4D-Content-Creation"><a href="#GaussianFlow-Splatting-Gaussian-Dynamics-for-4D-Content-Creation" class="headerlink" title="GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation"></a><a href="https://arxiv.org/abs/2403.12365">GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation</a></h1><ul><li>13 May 2024</li></ul><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Creating 4D fields of Gaussian Splatting from images or videos is a challenging task due to its under-constrained nature. <span style="color: red;"><strong>While the optimization can draw photometric reference from the input videos or be regulated by generative models, directly supervising Gaussian motions remains underexplored.</strong></span> In this paper, we introduce a novel concept, <span style="color: blue"><strong>Gaussian flow</strong></span>, <strong>which connects the dynamics of 3D Gaussians and pixel velocities between consecutive frames</strong>. </p><ul><li>The Gaussian flow can be efficiently obtained by <strong>splatting Gaussian dynamics into the image space</strong>. T</li><li>his differentiable process enables <strong>direct dynamic supervision from optical flow</strong>. </li><li>Our method significantly benefits 4D dynamic content <strong>generation</strong> and 4D novel view <strong>synthesis</strong> with Gaussian Splatting, especially for contents with rich motions that are hard to be handled by existing methods. </li><li>The common <strong>color drifting</strong> issue that happens in 4D generation is also resolved with improved Guassian dynamics. </li><li>Superior visual quality on extensive experiments demonstrates our method’s effectiveness. Quantitative and qualitative evaluations show that our method achieves state-of-the-art results on both tasks of 4D generation and 4D novel view synthesis.</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>4D dynamic content creation from monocular or multi-view videos has garnered significant attention from academia and industry due to its wide applicability in virtual reality/augmented reality, digital games, and movie industry. Studies model 4D scenes by 4D dynamic <strong>Neural Radiance Fields (NeRFs)</strong> and optimize them based on input multi-view or monocular videos. Once optimized, the 4D field can be viewed from novel camera poses at preferred time steps through volumetric rendering. <span style = "color: red"><strong>A more challenging task is generating 360 degree 4D content based on uncalibrated monocular videos or synthetic videos generated by text-to-video or image-to-video models.</strong></span> Since the monocular input <strong>cannot provide enough multi-view cues and unobserved regions are not supervised due to occlusions</strong>, studies optimizes 4D dynamic NeRFs by leveraging <strong>generative</strong> models to create plausible and temporally consistent 3D structures and appearance. </p><p><span style = "color: red;"><strong>The optimization of 4D NeRFs requires volumetric rendering which makes the process time-consuming. And real-time rendering of optimized 4D NeRFs is also hardly achieved without special designs.</strong></span> A more efficient alternative is to model 4D Radiance Fields by <span style = "color: blue;"><strong>4D Gaussian Splatting</strong></span> (GS), which extends 3D Gaussian Splatting with a temporal dimension. Leveraging the efficient rendering of 3D GS, <strong>the lengthy training time of a 4D Radiance Field can be drastically reduced and rendering can achieve real-time speed during inference.</strong> The optimization of 4D Gaussian fields takes photometric loss as major supervision. As a result, the scene dynamics are usually <strong>under-constraint</strong>. Similarly to 4D NeRFs, the radiance properties and the time-varying spatial properties (location, scales, and orientations) of Gaussians are both optimized to reduce the photometric Mean Squared Error (MSE) <strong>between the rendered frames and the input video frames</strong>. The <strong>ambiguities</strong> of appearance, geometry, and dynamics have been introduced in the process and become prominent with sparse-view or monocular video input. <strong>Per-frame Score Distillation Sampling (SDS) reduces the appearance-geometry ambiguity</strong> to some extent by involving multi-view supervision in latent domain. <span style = "color: red"><strong>However, both monocular photometric supervision and SDS supervision do not directly supervise scene dynamics.</strong></span> </p><p>To avoid temporal inconsistency brought by fast motions, Consistent4D leverages a video interpolation block, which imposes a photometric consistency between the interpolated frame and generated frame, at a cost of involving moreframes as pseudo ground truth for fitting. Similarly, AYG uses text-tovideo diffusion model to balance motion magnitude and temporal consistency with a pre-set frame rate. 4D NeRF model has proven that optical flows on reference videos are strong motion cues and can significantly benefit scene dynamics. <strong>However, for 4D GS, connecting 4D Gaussian motions with optical flows has following two challenges.</strong> </p><ul><li>First, a Gaussian’s <strong>motion</strong> is in <strong>3D space</strong>, but it is its <strong>2D splat that contributes to rendered pixels</strong>. </li><li>Second, <strong>multiple</strong> 3D Gaussians might contribute to the <strong>same</strong> pixel in rendering, and each <strong>pixel’s flow does not equal to any one Gaussian’s motion</strong>. </li></ul><p>To deal with these challenges, we introduce a novel concept, <span style = "color: blue;"><strong>Gaussian flow</strong></span>, <strong>bridging the dynamics of 3D Gaussians and pixel velocities between consecutive frames.</strong> </p><ul><li>Specifically, we assume the optical flow of each pixel in image space is influenced by the Gaussians that cover it. </li><li>The Gaussian flow of each pixel is considered to be the <strong>weighted sum</strong> of these Gaussian motions in 2D. </li><li>To obtain the Gaussian flow value on each pixel without losing the speed advantage of Gaussian Splatting, <strong>we splat 3D Gaussian dynamics</strong>, including scaling, rotation, and translation in 3D space, <strong>onto the image plane along with its radiance properties</strong>. </li><li>As the whole process is end-to-end <strong>differentiable</strong>, the <strong>3D Gaussian dynamics can be directly supervised by matching Gaussian flow with optical flow on input video frames</strong>. </li><li>We apply such flow supervision to both 4D content generation and 4D novel view synthesis to showcase the benefit of our proposed method, especially for contents with rich motions that are hard to be handled by existing methods. The flow-guided Guassian dynamics also resolve the color drifting artifacts that are commonly observed in 4D Generation. We summarize our contributions as follows:</li></ul><h1 id="Motion-aware-3D-Gaussian-Splatting-for-Efficient-Dynamic-Scene-Reconstruction"><a href="#Motion-aware-3D-Gaussian-Splatting-for-Efficient-Dynamic-Scene-Reconstruction" class="headerlink" title="Motion-aware 3D Gaussian Splatting for Efficient Dynamic Scene Reconstruction"></a><a href="https://arxiv.org/abs/2403.11447">Motion-aware 3D Gaussian Splatting for Efficient Dynamic Scene Reconstruction</a></h1><ul><li>18 Mar 2024</li></ul><h2 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h2><p>3D Gaussian Splatting (3DGS) has become an emerging tool for dynamic scene reconstruction. However, existing methods focus mainly on extending static 3DGS into a time-variant representation, <span style="color: red;"><strong>while overlooking the rich motion information carried by 2D observations</strong></span>, thus suffering from performance degradation and model redundancy. To address the above problem, we propose a novel <span style="color: blue;"><strong>motion-aware</strong></span> enhancement framework for dynamic scene reconstruction, which mines useful motion cues from optical flow to improve different paradigms of dynamic 3DGS.</p><ul><li>Specifically, we first establish a <strong>correspondence between 3D Gaussian movements and pixel-level flow</strong>.</li><li>Then a novel <strong>flow augmentation</strong> method is introduced with additional insights into <strong>uncertainty</strong> and <strong>loss collaboration</strong>.</li><li>Moreover, for the prevalent <strong>deformation-based</strong> paradigm that presents a harder optimization problem, a <strong>transient-aware deformation auxiliary module is proposed</strong>. </li><li>We conduct extensive experiments on both multi-view and monocular scenes to verify the merits of our work. Compared with the baselines, our method shows significant superiority in both rendering quality and efficiency.</li></ul><h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><p>While static scene modeling has witnessed significant progress in recent years, <strong>reconstructing dynamic scenes</strong> remains an intractable challenge due to the difficulties introduced by <span style="color: red;"><strong>motion complexities, topological changes, and spatially or temporally sparse observations</strong></span>.</p><p>In the past few years, <strong>Neural Radiance Fields</strong> (NeRF) has emerged as a remarkable implicit representation for 3D scenes. Despite their impressive visual quality, <span style="color: red;"><strong>the large time overhead stands as a non-negligible obstacle to their practical application</strong></span>. Recently, a new method named <strong>3D Gaussian Splatting</strong> (3DGS) has attracted substantial attention from the research community. 3DGS gets rid of the expensive deep neural networks and ray-tracing rendering of NeRF-based methods by introducing the explicit 3D Gaussian representation and efficient point-based rasterization. As a strong competitor to NeRF, 3DGS achieves comparable performance in novel view synthesis while <span style="color: blue;"> <strong>boosting the rendering speed to a real-time level</strong> </span>.</p><p><strong>It is then a straightforward but challenging task to extend the static 3DGS to a time-variant representation for dynamic content.</strong> Some pioneers have tried different strategies, e.g., <strong>iteration</strong> or <strong>deformation</strong>, to address this problem. However, these 3DGS-based works typically focus on the design of dynamic modeling. They regard frames as discrete samples to fit time-dependent trajectories or deformations, <span style = "color: red;"><strong>while overlooking the rich motion cues underneath sequential 2D observations</strong></span>. Since dynamic 3DGS involves explicit moving and deforming of Gaussians, it presents a tough <span style = "color: red;"><strong>under-constrained problem</strong></span> to use only images to supervise the reconstruction. <strong>The model is prone to local optimum where temporal consistency of Gaussians is not maintained as in physical world, especially for cases with insufficient viewpoints.</strong> This leads to visual overfitting, performance degradation, and redundant modeling in practice.</p><p><strong>To address the above issues, we propose a novel motion-aware framework to enhance dynamic 3DGS by taking full advantage of optical flow prior.</strong> </p><ul><li>As a well-explored representation of pixel-level movement, <span style = "color: blue;"><strong>optical flow can be efficiently predicted by pretrained networks</strong></span>, providing low-cost 2D motion prior for 3DGS. </li><li>Instead of using plausible render-based supervision like previous practices in depth or segmentation, we propose to establish a more robust and finer-grained cross-dimensional motion correspondence specially designed for flows. In this way, <span style = "color: blue;"><strong>Gaussian motions between frames can be aligned with 2D prior using our uncertainty-aware flow loss</strong>.</span></li><li>Meanwhile, We offer <span style = "color: blue;"> <strong>dynamic awareness to existing regularization in neural rendering with the help of flow prior</strong></span>, thereby giving special attention to the motion parts during optimization.</li><li>Moreover, the prevalent deformation-based paradigm for dynamic 3DGS is susceptible to 3D motion ambiguities when relying solely on relative flow constraints. Therefore, we propose an additional <span style = "color: blue;"> <strong>deformation auxiliary module to inject transient motion information into Gaussian features and improve the dynamic modeling</strong></span>.</li><li>The overall framework is proved by extensive experiments to be an effective enhancing solution for multi-view/monocular scenes, which possesses efficient dynamic modeling capabilities with less redundancy.</li></ul><h1 id="4D-Gaussian-Splatting-Towards-Efficient-Novel-View-Synthesis-for-Dynamic-Scenes"><a href="#4D-Gaussian-Splatting-Towards-Efficient-Novel-View-Synthesis-for-Dynamic-Scenes" class="headerlink" title="4D Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes"></a><a href="https://arxiv.org/abs/2402.03307">4D Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes</a></h1><ul><li>7 Feb 2024</li></ul><h2 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h2><p>We consider the problem of novel view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. <span style = "color: red;"><strong>Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or capturing high-fidelity renderings.</strong></span> In this paper, we introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic scenes with <span style = "color:blue"><strong>anisotropic 4D XY ZT Gaussians</strong> </span>, inspired by the success of 3D Gaussian Splatting in static scenes [26]. </p><ul><li>We model dynamics at each timestamp by temporally <strong>slicing</strong> the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. </li><li>As an explicit spatialtemporal representation, 4DGS demonstrates powerful capabilities for modeling complicated dynamics and fine details—especially for scenes with abrupt motions. </li><li>We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. </li><li>Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DGS, which consistently outperforms existing methods both quantitatively and qualitatively.</li></ul><h2 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h2><p>Reconstructing 3D scenes from 2D images and synthesizing their appearance from novel views has been a longstanding goal in computer vision and graphics. This task is pivotal in numerous industrial applications including film, gaming, and VR/AR, where there is a substantial demand for high-speed, photo-realistic rendering effects. The task diverges into two different scene types: <strong>static</strong> scenes where objects are still across all images [4, 24, 27, 37] and <strong>dynamic</strong> scenes where scene contents exhibit temporal variations [11, 33, 40, 43, 57]. While the former has witnessed significant progress recently, <span style = "color: red;"><strong>efficient and accurate NVS for dynamic scenes remains challenging due to the complexities introduced by the temporal dimension and diverse motion patterns</strong></span>. </p><p>A variety of methods have been proposed to tackle the challenges posed by dynamic NVS. </p><ul><li>A series of methods model the 3D scene and its dynamics jointly. <span style = "color: red;"><strong>However, these methods often fall short in preserving fine details in the NVS renderings due to the complexity caused by the highly entangled spatial and temporal dimensions.</strong></span></li><li>Alternatively, many existing techniques decouple dynamic scenes by learning a static canonical space and then predicting a deformation field to account for the temporal variations. <span style = "color: red;"><strong>Nonetheless, this paradigm struggles in capturing complex dynamics such as objects appearing or disappearing suddenly.</strong></span> </li><li>More importantly, prevailing methods on dynamic NVS mostly build upon volumetric rendering, which requires dense sampling on millions of rays. <span style = "color: red;"><strong>As a consequence, these methods typically cannot supportreal-time rendering speed even for static scenes.</strong></span> </li></ul><p>Recently, <strong>3D Gaussian Splatting (3DGS)</strong>  has emerged as a powerful tool for efficient NVS of static scenes. By explicitly modeling the scene with 3D Gaussian ellipsoids and employing fast rasterization technique, it achieves photo-realistic NVS in real time. <span style = "color: blue;"><strong>Inspired by this, we propose to lift Gaussians from 3D to 4D and provide a novel spatial-temporal representation that enables NVS for more challenging dynamic scenes</strong></span>. </p><p><span style = "color: blue;"> <strong>Our key observation is that 3D scene dynamics at each timestamp can be viewed as 4D spatial-temporal Gaussian ellipsoids sliced with different time queries.</strong></span> The dynamics in 2D XY space at time $T_i$ is equivalent to building 3D XYT Gaussians and slicing by the $t = T_i$ plane. Analogously, we extend 3D Gaussians to 4D XYZT space to model dynamic 3D scenes. The temporally sliced 4D Gaussians compose 3D Gaussians that can be seamlessly projected to 2D screens via fast rasterization, inheriting both exquisite rendering effects and high speed characteristic from 3DGS. Moreover, extending the prune-split mechanism in the temporal dimension makes 4D Gaussians particularly suitable for <strong>representing complex dynamics, including abrupt appearances or disappearances</strong>.</p><p>It is non-trivial to lift 3D Gaussians into 4D space, where tremendous challenges exist in the design of the <strong>4D</strong> <strong>rotation</strong>, <strong>slicing</strong>, as well as the <strong>joint spatial-temporal optimization scheme</strong>. <span style = "color: blue;"><strong>We draw inspiration from geometric algebra and carefully choose 4D rotor to represent 4D rotation, which is a spatial-temporal separable rotation representation</strong></span>. Notably, rotor representation accommodates both 3D and 4D rotation: when the temporal dimension is set to zero, it becomes equivalent to a quaternion and can represent 3D spatial rotation as well. Such adaptability grants our method the flexibility to model both dynamic and static scenes. In other words, 4DGS is a generalizable form of 3DGS: <strong>when closing the temporal dimension, our 4DGS reduces to 3DGS</strong>. </p><p><span style = "color: blue;"> <strong>We enhance the optimization strategies in 3DGS and introduce two new regularization terms to stabilize and improve the dynamic reconstruction</strong></span>.We first propose an <strong>entropy loss</strong> that pushes the opacity of Gaussians towards either one or zero, which proves effective to remove “floaters” in our experiments. We further introduce a novel <strong>4D consistency loss</strong> to regularize the motion of Gaussian points and yield more consistent dynamics reconstruction. Experiments show that both terms notably improve the rendering quality. </p><p>While existing Gaussian-based methods are mostly based on PyTorch, we further develop a highly optimized CUDA framework with careful engineering designs for fast training and inference speed. Our framework supports rendering 1352×1014 videos at an unprecedented 583 FPS on an RTX 4090 GPU and 277 FPS on an RTX 3090 GPU. We conduct extensive experiments on two datasets spanning a wide range of settings and motion patterns, including monocular videos and multicamera videos. Quantitative and qualitative evaluations demonstrate the distinct advantages over preceding methods, including the new state-of-the-art rendering quality and speed.</p><h1 id="4D-Gaussian-Splatting-for-Real-Time-Dynamic-Scene-Rendering"><a href="#4D-Gaussian-Splatting-for-Real-Time-Dynamic-Scene-Rendering" class="headerlink" title="4D Gaussian Splatting for Real-Time Dynamic Scene Rendering"></a><a href="https://arxiv.org/abs/2310.08528">4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</a></h1><ul><li>7 Dec 2023</li></ul><h2 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h2><p>Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to guarantee. <strong>To achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency</strong>, we propose <span style ="color:blue;"><strong>4D Gaussian Splatting</strong></span> (4D-GS) as a holistic representation for dynamic scenes rather than applying 3D-GS for each individual frame. </p><ul><li>In 4D-GS, a novel explicit representation containing both 3D Gaussians and 4D neural voxels is proposed. </li><li>A decomposed neural voxel encoding algorithm inspired by HexPlane is proposed to efficiently build Gaussian features from 4D neural voxels and then a lightweight MLP is applied to predict Gaussian deformations at novel timestamps.</li><li>Our 4D-GS method achieves real-time rendering under high resolutions, 82 FPS at an 800×800 resolution on an RTX 3090 GPU while maintaining comparable or better quality than previous state-of-the-art methods. More demos and code are available at <a href="https://guanjunwu.github.io/4dgs/">https://guanjunwu.github.io/4dgs/</a>.</li></ul><h2 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h2><p>Novel view synthesis (NVS) stands as a critical task in the domain of 3D vision and plays a vital role in many applications, e.g. VR, AR, and movie production. NVS aims at rendering images from any desired viewpoint or timestamp of a scene, usually requiring modeling the scene accurately from several 2D images. Dynamic scenes are quite common in real scenarios, rendering which is important but challenging as <span style ="color:red;"><strong>complex motions need to be modeled with both spatially and temporally sparse input</strong></span>. </p><p><strong>NeRF</strong> has achieved great success in synthesizing novel view images by representing scenes with implicit functions. The volume rendering techniques are introduced to connect 2D images and 3D scenes. However, the original NeRF method bears big training and rendering costs. Though some NeRF variants reduce the training time from days to minutes, the rendering process still bears a <span style ="color:red;"><strong>non-negligible latency</strong></span>.</p><p>Recent <strong>3D Gaussian Splatting</strong> (3D-GS) significantly boosts the rendering speed to a real-time level by representing the scene as 3D Gaussians. The cumbersome volume rendering in the original NeRF is replaced with efficient differentiable splatting, which directly projects 3D Gaussian points onto the 2D plane. <strong>3D-GS not only enjoys real-time rendering speed but also represents the scene more explicitly, making it easier to manipulate the scene representation.</strong></p><p><strong>However, 3D-GS focuses on the static scenes. Extending it to dynamic scenes as a 4D representation is a reasonable, important but difficult topic.</strong> <span style ="color: red;">**The key challenge lies in modeling complicated point motions from sparse input.</span><strong> 3DGS holds a natural geometry prior by representing scenes with point-like Gaussians. One direct and effective extension approach is to </strong>construct 3D Gaussians at each timestamp<strong> but <span style="color:red;"></strong>the storage/memory cost will multiply especially for long input sequences<strong>&lt;/span&gt;. Our goal is to construct a </strong>compact representation while maintaining both training and rendering efficiency<strong>, i.e. <span style="color:blue"></strong>4D Gaussian Splatting (4DGS)**&lt;/span&gt;. </p><ul><li>To this end, we propose to represent Gaussian motions and shape changes by an efficient <strong>Gaussian deformation field network</strong>, containing a <strong>temporal-spatial structure encoder</strong> and an extremely tiny <strong>multi-head Gaussian deformation decoder</strong>. </li><li><strong>Only one set of canonical 3D Gaussians</strong> is maintained. </li><li>For each timestamp, the <strong>canonical 3D Gaussians</strong> will be transformed by the <strong>Gaussian deformation field</strong> into new positions with new shapes. The <strong>transformation</strong> process represents both the Gaussian <strong>motion</strong> and <strong>deformation</strong>. </li><li>Note that different from modeling motions of each Gaussian separately, the spatial-temporal structure encoder can connect different <strong>adjacent</strong> 3D Gaussians to predict <strong>more accurate motions and shape deformation</strong>. Then the deformed 3D Gaussians can be directly splatted for rendering the according-timestamp image.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Dynamic Gaussian Splatting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Papers </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>This is the Start of My Blog</title>
      <link href="/2024/06/02/start/"/>
      <url>/2024/06/02/start/</url>
      
        <content type="html"><![CDATA[<p>Welcome to my blog! Maybe we should call it the Progress Report.</p><p>As you can see, I’m not particularly skilled at writing in English. Yet, I’ve decided to document the little things in my life in English. Here are some reasons why:</p><p>This blog, along with future ones, is meant to be a tool for practicing English. It’s also a progress report on my English writing skills. Inspired by ``Flowers for Algernon’’, I believe that the evolution of my language and writing will reflect changes in my mental state and cognitive abilities, which should be quite fascinating. Because writing in English forces me to focus on the clarity and logical structure of my sentences rather than on elegant phrasing. Through this straightforward but challenging method, I hope to concentrate more on my thoughts about life and better understand the changes in my psychological state.</p><p>Often, when we are eager to achieve something or reach a certain mental state, we might end up doing the opposite. This idea is presented in ``The Glass Bead Game’’, where the music master advises the protagonist to take time for meditation to get out of this situation. However, given my understanding of my own intellectual capabilities, meditation doesn’t seem a very practical solution for me. My mind is not able to handle multiple things in a multi-threaded manner, and it is also difficult for me to focus on a single task for a long time. I often blur or even forget my previous insights after a lot of thinking. Therefore, why not record some of these thoughts and insights instead of just leaving them in the brain? Not only will this help clarify my current reality, but it will also serve as valuable material for future self-reflection. As mentioned earlier, this provides excellent feedback on my cognitive development and its evolution.</p><p>To better achieve this goal, I plan to conduct a weekly summary. Regular self-examination will help me understand how my psychological state evolves over different periods and the impact of continuous time sequences on my thoughts. Whether I can stick to this schedule remains to be seen.</p><p>At this stage, my work is becoming increasingly busy. I’m unsure how current events will unfold in the future. This includes a paper I’m submitting, my new advisor at NUS, my completely new research direction, and the development of relationships with new acquaintances. All these stem from my bold decision to abandon my guaranteed postgraduate spot and pursue overseas studies—a choice not many have made, offering few precedents. The past years’ pandemic effects and changing international dynamics further obscure my decisions’ future outcomes, making it hard to calculate the probability of achieving my planned goals. The world is complex, like the veil of Maya, obscuring our vision and understanding. We can’t see the true nature of the world, just as a sandy ground in the sunlight looks like water from afar. This uncertainty brings me anxiety. I know that excessive worry about the future isn’t beneficial, but not worrying makes me fear missing opportunities. I realize I might be exaggerating; perhaps these are just trivial matters. Ultimately, I can’t predict the future, but I can only try to minimize potential risks. This state is exhausting, but viewed differently, it’s a trade of hope for what I aim to achieve. I hope it will broaden my perspective, though it might also lead to stressful situations. Maybe this blog will help.</p><p>Years from now, when I look back at this blog, I’ll probably laugh. I hope so—that would mean I’ve made progress. And that’s precisely the purpose of these blogs.</p><p>如你所见，我并不是很擅长英语写作。但我还是决定以英语为主来记录生活中的一些琐事。为什么呢？原因如下：</p><p>这个博客，包括未来的博客，可以作为一种英语练习工具，也可以看作是我英语写作进步的记录。这灵感来自于《献给阿尔吉侬的花束》。通过文字的变化，反映出写作技巧和精神状态、思想认知水平的变化，这应该会非常有趣。</p><p>写英语能让我更加关注句子的表意和句子之间的逻辑结构，而不是华丽的辞藻。因此，我希望通过这种对我来说并不简单的方式，让我更专注于对生活的思考，并进一步了解自己的心理变化。《玻璃球游戏》中提到，当我们迫切想实现某件事或追求某种心理状态时，往往会适得其反。书中的音乐大师建议主人公花时间冥想，但我知道冥想对我来说并不实际。我的思维无法满足多线程处理多件事情，也同样难以长时间专注于单一任务，常常会在大量思考后模糊甚至遗忘之前的见解。所以，记录一些想法和见解不仅能帮助我理清当前的现实，也是一份未来回顾自己的宝贵材料。这对个人思想认知及其变化是一种很好的反馈。</p><p>为了更好地实现这个目标，通过定期自我审视，理清现实对我在不同时间段产生的心理认知影响，以及连续时间对我思想的作用及其变化，我觉得每周至少做一次总结是一个不错的方式。但谁知道我能否坚持呢？</p><p>现阶段工作越来越忙，我也不清楚未来会如何演变。这包括我的一篇在投论文、新加坡国立大学的新导师、全新的研究方向（之前没有任何基础的研究方向），以及新认识的人的关系发展等等。这一切都源于我一个大胆的决定：在保研之前放弃保研，转而出国留学。之前并没有很多人这样做，这样的决策参考实例很少，再加上疫情影响和国际局势的变化，我无法对自己的决定做出清晰的未来预测，也导致我期望的规划完成概率难以计算。世界如此复杂，就像摩耶之纱蒙住我们的双眼，我们无法清晰认识世界的本质。就像阳光下的沙地，远看像水，我们无法看透。这带给我焦虑，但我知道过分担忧未来并不好，但如果不焦虑，我又害怕会错过时机。</p><p>可能说得有些大了，也许这些都是些芝麻大的小事也说不定。总之，我无法预测未来，但我只能尽量降低潜在风险，这样的状态很累。但从另一个角度看，这是一种用希望做交易的方式。我期待这能换来更多的思想与认识，也许这会让我看得更远一些，但同时也可能因为急切与忙碌陷入困境。希望博客能帮上忙。</p><p>多年以后，当我回头看这篇博客时，一定会笑出来的。但愿如此，这也是我进步的反馈，这正是这些博客的初衷。</p>]]></content>
      
      
      <categories>
          
          <category> Diary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Life </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/06/02/hello-world/"/>
      <url>/2024/06/02/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> Tech </category>
          
          <category> Website </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
